{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565b9e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download_path: C:\\Users\\USER/.insightface\\models\\buffalo_l\n",
      "Downloading C:\\Users\\USER/.insightface\\models\\buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281857/281857 [01:23<00:00, 3375.76KB/s]\n",
      "d:\\facial finale\\my_face_zindabad\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "Detected 1 face(s).\n",
      "Face 1 bounding box: (1218, 592), (1806, 1363)\n"
     ]
    }
   ],
   "source": [
    "from insightface.app import FaceAnalysis\n",
    "import cv2\n",
    "\n",
    "# Load InsightFace face detector\n",
    "app = FaceAnalysis(name='buffalo_l')  # loads ArcFace + detector + landmarks\n",
    "app.prepare(ctx_id=0)  # 0 = CPU (use 0 unless you're using GPU)\n",
    "\n",
    "# Load your image\n",
    "image_path = \"test_images\\IMG20250509145437.jpg\"  # Replace with your actual path\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Run face detection\n",
    "faces = app.get(img)\n",
    "\n",
    "# Check how many faces were found\n",
    "print(f\"Detected {len(faces)} face(s).\")\n",
    "\n",
    "# Loop through faces and print bounding boxes\n",
    "for i, face in enumerate(faces):\n",
    "    x1, y1, x2, y2 = face.bbox.astype(int)\n",
    "    print(f\"Face {i+1} bounding box: ({x1}, {y1}), ({x2}, {y2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0ebea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "from insightface.app import FaceAnalysis\n",
    "import cv2\n",
    "\n",
    "# Step 1: Load the InsightFace model\n",
    "app = FaceAnalysis(name='buffalo_l')\n",
    "app.prepare(ctx_id=0)  # 0 = CPU, or use 1 for GPU if configured\n",
    "\n",
    "# Step 2: Load your test image\n",
    "image_path = \"test_images\\IMG20250509145440.jpg\"  # Replace with your image\n",
    "img = cv2.imread(image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Step 3: Run face detection\n",
    "faces = app.get(img_rgb)\n",
    "\n",
    "# Step 4: Draw bounding boxes\n",
    "for face in faces:\n",
    "    x1, y1, x2, y2 = face.bbox.astype(int)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img, \"Face\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "# Step 5: Show the result\n",
    "# Step 5: Resize and show the result\n",
    "scale_percent = 30  # Change this value to scale the image\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "resized_img = cv2.resize(img, (width, height))\n",
    "cv2.imshow(\"Face Detection\", resized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b67cced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from insightface.app import FaceAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7510766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\USER/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Setup InsightFace\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])  # Use ['CUDAExecutionProvider'] if you have GPU\n",
    "app.prepare(ctx_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c98f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory\n",
    "root_dir = \"frames\"\n",
    "splits = [\"train\", \"test\", \"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b9c74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/01: 100%|██████████| 700/700 [04:56<00:00,  2.36it/s]\n",
      "train/02: 100%|██████████| 700/700 [05:01<00:00,  2.32it/s]\n",
      "train/03: 100%|██████████| 700/700 [05:01<00:00,  2.32it/s]\n",
      "train/04: 100%|██████████| 700/700 [04:56<00:00,  2.36it/s]\n",
      "train/05: 100%|██████████| 700/700 [05:00<00:00,  2.33it/s]\n",
      "train/06: 100%|██████████| 700/700 [05:01<00:00,  2.32it/s]\n",
      "train/07: 100%|██████████| 700/700 [04:59<00:00,  2.33it/s]\n",
      "test/01: 100%|██████████| 200/200 [01:25<00:00,  2.34it/s]\n",
      "test/02: 100%|██████████| 200/200 [01:25<00:00,  2.33it/s]\n",
      "test/03: 100%|██████████| 200/200 [01:24<00:00,  2.36it/s]\n",
      "test/04: 100%|██████████| 200/200 [01:24<00:00,  2.35it/s]\n",
      "test/05: 100%|██████████| 200/200 [01:25<00:00,  2.34it/s]\n",
      "test/06: 100%|██████████| 200/200 [01:25<00:00,  2.33it/s]\n",
      "test/07: 100%|██████████| 200/200 [01:24<00:00,  2.36it/s]\n",
      "valid/01: 100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n",
      "valid/02: 100%|██████████| 100/100 [00:42<00:00,  2.35it/s]\n",
      "valid/03: 100%|██████████| 100/100 [00:42<00:00,  2.34it/s]\n",
      "valid/04: 100%|██████████| 100/100 [00:42<00:00,  2.35it/s]\n",
      "valid/05: 100%|██████████| 100/100 [00:42<00:00,  2.38it/s]\n",
      "valid/06: 100%|██████████| 100/100 [00:42<00:00,  2.37it/s]\n",
      "valid/07: 100%|██████████| 100/100 [00:42<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Helper: Convert bbox to YOLO format\n",
    "def convert_to_yolo(bbox, img_width, img_height):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x_center = (x1 + x2) / 2.0 / img_width\n",
    "    y_center = (y1 + y2) / 2.0 / img_height\n",
    "    w = (x2 - x1) / img_width\n",
    "    h = (y2 - y1) / img_height\n",
    "    return [x_center, y_center, w, h]\n",
    "\n",
    "# Go through each split\n",
    "for split in splits:\n",
    "    split_path = os.path.join(root_dir, split)\n",
    "\n",
    "    for person_folder in sorted(os.listdir(split_path)):\n",
    "        person_path = os.path.join(split_path, person_folder)\n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            class_id = int(person_folder) - 1  # \"01\" → 0, \"02\" → 1, etc.\n",
    "        except ValueError:\n",
    "            print(f\"Skipping non-numeric folder: {person_folder}\")\n",
    "            continue\n",
    "\n",
    "        for img_file in tqdm(os.listdir(person_path), desc=f\"{split}/{person_folder}\"):\n",
    "            if not img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(person_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            faces = app.get(img)\n",
    "            h, w = img.shape[:2]\n",
    "            yolo_lines = []\n",
    "\n",
    "            for face in faces:\n",
    "                bbox = face.bbox.astype(int)\n",
    "                yolo_bbox = convert_to_yolo(bbox, w, h)\n",
    "                line = f\"{class_id} {' '.join(map(str, yolo_bbox))}\"\n",
    "                yolo_lines.append(line)\n",
    "\n",
    "            label_filename = os.path.splitext(img_file)[0] + \".txt\"\n",
    "            label_path = os.path.join(person_path, label_filename)\n",
    "\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(yolo_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8bf065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_root = 'frames'\n",
    "target_root = 'dataset'  # this will be created\n",
    "\n",
    "splits = ['train', 'test', 'valid']\n",
    "\n",
    "for split in splits:\n",
    "    image_dir = os.path.join(target_root, 'images', split)\n",
    "    label_dir = os.path.join(target_root, 'labels', split)\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "    split_path = os.path.join(source_root, split)\n",
    "    if not os.path.exists(split_path):\n",
    "        continue\n",
    "\n",
    "    for person_folder in os.listdir(split_path):\n",
    "        person_path = os.path.join(split_path, person_folder)\n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "        for file in os.listdir(person_path):\n",
    "            src_path = os.path.join(person_path, file)\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            if ext in ['.jpg', '.png', '.jpeg']:\n",
    "                dst_path = os.path.join(image_dir, file)\n",
    "            elif ext == '.txt':\n",
    "                dst_path = os.path.join(label_dir, file)\n",
    "            else:\n",
    "                continue\n",
    "            shutil.copyfile(src_path, dst_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_face_zindabad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
